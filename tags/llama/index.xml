<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLaMA on devbins blog</title>
    <link>http://devbins.github.io/tags/llama/</link>
    <description>Recent content in LLaMA on devbins blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://devbins.github.io/tags/llama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mac 部署LLaMA2</title>
      <link>http://devbins.github.io/post/llama/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>http://devbins.github.io/post/llama/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;前不久 &lt;code&gt;Meta&lt;/code&gt; 开源了&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;Llama 2 - Meta AI&lt;/a&gt; ，并且是可商用的。&lt;/p&gt;
&lt;p&gt;所以今天要在 &lt;code&gt;Mac&lt;/code&gt; 上搭建 &lt;code&gt;Llama&lt;/code&gt; ，由于 &lt;code&gt;Llama&lt;/code&gt; 需要非常高的内存普通玩家玩不起，所以为了在 &lt;code&gt;Mac&lt;/code&gt; 上跑起来就有了&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/tree/master&#34;&gt;ggerganov/llama.cpp: Port of Facebook&amp;rsquo;s LLaMA model in C/C++&lt;/a&gt; ，本文也是采用 &lt;code&gt;llama.cpp&lt;/code&gt; 的方式进行部署。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
