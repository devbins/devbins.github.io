<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on devbins blog</title>
    <link>http://localhost:1313/categories/ai/</link>
    <description>Recent content in AI on devbins blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 21 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>在Ollama 上运行 DeepSeek 大模型</title>
      <link>http://localhost:1313/post/ollama_deepseek/</link>
      <pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/ollama_deepseek/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;今天在群里有个小伙伴发了一个在 &lt;code&gt;Ollama&lt;/code&gt; 上运行 &lt;code&gt;deepseek&lt;/code&gt; 大模型的消息，最近 &lt;code&gt;deepkseek&lt;/code&gt; 也是风头正劲，于是我就体验了一下。&lt;/p&gt;&#xA;&lt;p&gt;发现了还不错，它相比于其它的大模型会把思考过程也显示出来。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gemini搭建</title>
      <link>http://localhost:1313/post/gemini/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/gemini/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;Google 发布了 &lt;code&gt;Gemoni&lt;/code&gt; 模型，并且提供了 &lt;code&gt;API&lt;/code&gt; ，这就非常有意思了，我们可以根据 &lt;code&gt;Google&lt;/code&gt; 提供的 &lt;code&gt;API&lt;/code&gt; 来开发自己的应用。&lt;/p&gt;&#xA;&lt;p&gt;就在我要准备看 &lt;code&gt;API&lt;/code&gt; 的时候，转念一想估计已经有人做出来了，果不其然，一搜就搜到了。&lt;/p&gt;</description>
    </item>
    <item>
      <title>WebUI尝鲜LCM采样器</title>
      <link>http://localhost:1313/post/webui_lcm/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/webui_lcm/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;前些日子清华大学发布了 &lt;code&gt;LCM&lt;/code&gt; 采样器，能够极大的提升出图速度， &lt;code&gt;ComfyUI&lt;/code&gt; 已经用上了，但是 &lt;code&gt;WebUI&lt;/code&gt; 却没有。&lt;/p&gt;&#xA;&lt;p&gt;这篇文章就是写给想要提前尝鲜的朋友。&lt;/p&gt;&#xA;&lt;h2 id=&#34;增加采样器&#34;&gt;增加采样器&lt;/h2&gt;&#xA;&lt;p&gt;在改之前先提醒一下，这里需要修改代码，如果你是使用 &lt;code&gt;Git&lt;/code&gt; 的那么无所谓，改了可以恢复，如果不是请做好备份。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mac 部署LLaMA2</title>
      <link>http://localhost:1313/post/llama/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/llama/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;前不久 &lt;code&gt;Meta&lt;/code&gt; 开源了&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;Llama 2 - Meta AI&lt;/a&gt; ，并且是可商用的。&lt;/p&gt;&#xA;&lt;p&gt;所以今天要在 &lt;code&gt;Mac&lt;/code&gt; 上搭建 &lt;code&gt;Llama&lt;/code&gt; ，由于 &lt;code&gt;Llama&lt;/code&gt; 需要非常高的内存普通玩家玩不起，所以为了在 &lt;code&gt;Mac&lt;/code&gt; 上跑起来就有了&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/tree/master&#34;&gt;ggerganov/llama.cpp: Port of Facebook&amp;rsquo;s LLaMA model in C/C++&lt;/a&gt; ，本文也是采用 &lt;code&gt;llama.cpp&lt;/code&gt; 的方式进行部署。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mac 体验ChatGLM2-6B</title>
      <link>http://localhost:1313/post/chatglm/</link>
      <pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/chatglm/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;&#xA;&lt;p&gt;前不久换了新 &lt;code&gt;Mac&lt;/code&gt; ，性能还可以，于是就想试着体验一下 &lt;code&gt;ChatGLM&lt;/code&gt; ，看看能不能跑起来。&lt;/p&gt;&#xA;&lt;p&gt;所以今天就来体验一下 &lt;code&gt;ChatGLM2-6B&lt;/code&gt; 。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
